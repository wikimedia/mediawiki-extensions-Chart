Phabricator: [T370378](https://phabricator.wikimedia.org/T370378)

Status: Implemented

Date: September 20, 2024
Updated as amended per implementation: December 5, 2024

# Problem statement

How should usage of `Data:` pages for chart definitions and tabular data be tracked, with support across the Wikimedia farm for invalidating cached pages on update?

# Decision outcome

After some discussion we decided not to make a more general multiple-type tracking solution, but to make one specific to `JsonConfig`-backed page consumption, of which Chart is the primary and initial consumer.

Most of this work will live in `JsonConfig`, and will be documented there as well.

Local (same-wiki) data usages will be tracked via `templatelinks` as though they were transclusions; this provides local dependency tracking through MediaWiki's existing systems and requires no extra per-wiki setup.

Remote data usages will be tracked in a set of shared database tables, which can live in a separate database server such as `x1` in production without being joined against any of the other MediaWiki tables. These will be created once globally, and do not require any manual per-wiki setup or maintenance.

This allows MediaWiki on Commons to queue a cleanup job after updating or deleting any `Data:` page and fire off a cache purge, allowing the page to be regenerated with fresh data.

Proposed database schema (note that namespace IDs are stored as the local integer codes, and can only be interpreted within its own context):

```lang=sql
-- This has the same schema as the linktarget table, but since we intend for globaljsonlinks to be in x1, we can't join against linktarget
CREATE TABLE globaljsonlinks_target (
    gjlt_id BIGINT UNSIGNED AUTO_INCREMENT NOT NULL,
    gjlt_namespace INT NOT NULL,
    gjlt_title VARBINARY(255) NOT NULL,
    UNIQUE INDEX gjlt_namespace_title (gjlt_namespace, gjlt_title),
    PRIMARY KEY (gjlt_id)
);

CREATE TABLE globaljsonlinks_wiki (
    gjlw_id UNSIGNED AUTO_INCREMENT NOT NULL,
    gjlw_wiki VARBINARY(32) NOT NULL,
    UNIQUE INDEX gjlw_wiki_id (gjlw_wiki),
    PRIMARY KEY (gjlw_id)
);

CREATE TABLE globaljsonlinks (
    gjl_wiki UNSIGNED NOT NULL, /* refers to globaljsonlinks_wiki.gjlw_id */
    gjl_namespace INT NOT NULL,
    gjl_title VARBINARY(255) NOT NULL,
    gjl_target BIGINT UNSIGNED NOT NULL, /* refers to globaljsonlinks_target.gjlt_id */
    INDEX gjl_target_wiki_namespace_title (gjl_target, gjl_wiki, gjl_namespace, gjl_title),
    PRIMARY KEY (gjl_wiki, gjl_namespace, gjl_title, gjl_target)
);
```

The actual change propagation mechanism is implemented as:

* On update, Commons-side job is queued and may be lazy-run immediately
  * For each backlink recorded in `globaljsonlinks` for the updated page, a job is queued on the linking wiki's queue to run an html cache-invalidation purge on them
  * these then get run in the context of each individual wiki for local processing, purging HTTP caches and updating `page_touched` etc

# Decision drivers

* Adding new tables is extra trouble for production. A single shared table is cleaner, and use of existing tables where they are suitable is good.
* While we have several distinct tracking systems and were tempted to consider merging them into one big one, they are distinct for a number of good reasons as they are often _slightly different_ in schema details, usage patterns, size concerns, etc. Making a new system for this specific usage avoids having to store a 'type' parameter or handle heavier levels of traffic of other kinds of data. We don't distinguish between different types of `Data:` pages here or different types of consumption, but are aiming at simply tracking consumption and propagating invalidations.
* Using a set of tables on a separate shared database allows for flexibility of DBA details, and makes it easy to query the same data from Commons and from the other wikis. However it does mean we can't reuse common "target" tables for other link tables; this is not expected to be a huge driver of space compared to other stuff here.
* `globaljsonlinks_target` allows conserving space when the same definitions of data sets are reused many times, as expected. There is a possibility of harmlessly leaking old target records if we don't actively prune them, taking up space.
* `globaljsonlinks_wiki` squashes string names to a nice integer
* `globaljsonlinks` matches up the smaller wiki IDs with the `Data:` target page IDs from the other two tables via ns/title text of the linking page. This should be quite compact for widely used Data: pages, and makes it relatively easy to get a list of remote wikis to target (additional complexity: have to determine namespace text forms).

# Related questions for later

* Will we also track usage from the Lua tabular data loader and other such bits in `JsonConfig`?
  * Went ahead and implemented it this way as it was easy.
* Fixes were also required to use WAN cache in JsonConfig in order to propagate deletions of old cached data.
